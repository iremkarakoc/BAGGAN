{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation of the Working Environment\n",
        "---\n",
        "To run the StyleGAN2-ADA model with the specified requirements, you need:\n",
        "\n",
        "**Hardware:**\n",
        "\n",
        ">* 1 to 8 high-performance NVIDIA GPUs with at least 12GB of memory.\n",
        ">* Testing and development done with NVIDIA DGX-1 with 8 Tesla V100 GPUs.\n",
        "\n",
        "**Yazılım:**\n",
        "\n",
        ">* 64-bit Python 3.7\n",
        ">* PyTorch 1.7.1\n",
        ">* CUDA toolkit 11.0 or a newer version:"
      ],
      "metadata": {
        "id": "NSDJP_njWyZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "_ga7CqyaYhbq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c8e8ebd-8911-40d1-9db6-97ee8554d6ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install python3.7"
      ],
      "metadata": {
        "id": "LYCdbwFTEAzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ae15ff-7bc8-450c-e3c0-47a7bc70c2ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython3.7-minimal libpython3.7-stdlib python3.7-minimal\n",
            "Suggested packages:\n",
            "  python3.7-venv binfmt-support\n",
            "The following NEW packages will be installed:\n",
            "  libpython3.7-minimal libpython3.7-stdlib python3.7 python3.7-minimal\n",
            "0 upgraded, 4 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 4,530 kB of archives.\n",
            "After this operation, 23.3 MB of additional disk space will be used.\n",
            "Get:1 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 libpython3.7-minimal amd64 3.7.16-1+focal1 [588 kB]\n",
            "Get:2 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.7-minimal amd64 3.7.16-1+focal1 [1,808 kB]\n",
            "Get:3 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 libpython3.7-stdlib amd64 3.7.16-1+focal1 [1,773 kB]\n",
            "Get:4 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.7 amd64 3.7.16-1+focal1 [360 kB]\n",
            "Fetched 4,530 kB in 3s (1,352 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpython3.7-minimal:amd64.\n",
            "(Reading database ... 122545 files and directories currently installed.)\n",
            "Preparing to unpack .../libpython3.7-minimal_3.7.16-1+focal1_amd64.deb ...\n",
            "Unpacking libpython3.7-minimal:amd64 (3.7.16-1+focal1) ...\n",
            "Selecting previously unselected package python3.7-minimal.\n",
            "Preparing to unpack .../python3.7-minimal_3.7.16-1+focal1_amd64.deb ...\n",
            "Unpacking python3.7-minimal (3.7.16-1+focal1) ...\n",
            "Selecting previously unselected package libpython3.7-stdlib:amd64.\n",
            "Preparing to unpack .../libpython3.7-stdlib_3.7.16-1+focal1_amd64.deb ...\n",
            "Unpacking libpython3.7-stdlib:amd64 (3.7.16-1+focal1) ...\n",
            "Selecting previously unselected package python3.7.\n",
            "Preparing to unpack .../python3.7_3.7.16-1+focal1_amd64.deb ...\n",
            "Unpacking python3.7 (3.7.16-1+focal1) ...\n",
            "Setting up libpython3.7-minimal:amd64 (3.7.16-1+focal1) ...\n",
            "Setting up python3.7-minimal (3.7.16-1+focal1) ...\n",
            "Setting up libpython3.7-stdlib:amd64 (3.7.16-1+focal1) ...\n",
            "Setting up python3.7 (3.7.16-1+focal1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for mime-support (3.64ubuntu1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1\n",
        "\n"
      ],
      "metadata": {
        "id": "AURRNv0e8R73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo update-alternatives --config python3\n"
      ],
      "metadata": {
        "id": "vq4U8dyv8bEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3551f92d-13c6-4b0d-89d8-bb0ba085bbd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 3 choices for the alternative python3 (providing /usr/bin/python3).\n",
            "\n",
            "  Selection    Path                 Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/bin/python3.10   2         auto mode\n",
            "  1            /usr/bin/python3.10   2         manual mode\n",
            "  2            /usr/bin/python3.7    1         manual mode\n",
            "  3            /usr/bin/python3.8    1         manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/bin/python3.7 to provide /usr/bin/python3 (python3) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "zktyzk3n9SQE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feae5a14-bb19-4e80-ca87-b70e8637406a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install python3-pip\n"
      ],
      "metadata": {
        "id": "XfQ2-8CLNRiz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a16181-4cde-4ca3-fd7a-d29346437747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3-pip is already the newest version (20.0.2-5ubuntu1.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo mv /usr/lib/python3.7/distutils/ /usr/lib/python3.7/distutils_back\n",
        "!sudo ln -s /usr/lib/python3.8/distutils /usr/lib/python3.7/"
      ],
      "metadata": {
        "id": "bG2ndtyRLrta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip uninstall torch torchvision -y\n",
        "!python3 -m pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3\n",
        "!python3 -m pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!python3 -m pip install psutil\n",
        "!python3 -m pip install scipy\n",
        "!sudo apt-get install python3-dev"
      ],
      "metadata": {
        "id": "oDFNU-Uh1sBa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b44e913f-0536-4fc8-b19c-33dad132734e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\n",
            "\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting click\n",
            "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
            "Collecting requests\n",
            "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Collecting tqdm\n",
            "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "Collecting pyspng\n",
            "  Downloading pyspng-0.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
            "\u001b[K     |████████████████████████████████| 205 kB 12.7 MB/s \n",
            "\u001b[?25hCollecting ninja\n",
            "  Using cached ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "Collecting imageio-ffmpeg==0.4.3\n",
            "  Using cached imageio_ffmpeg-0.4.3-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Using cached certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
            "Collecting charset-normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (195 kB)\n",
            "\u001b[K     |████████████████████████████████| 195 kB 104.4 MB/s \n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
            "Collecting urllib3<3,>=1.21.1\n",
            "  Using cached urllib3-2.0.2-py3-none-any.whl (123 kB)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3 MB 64.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: click, certifi, charset-normalizer, idna, urllib3, requests, tqdm, numpy, pyspng, ninja, imageio-ffmpeg\n",
            "Successfully installed certifi-2023.5.7 charset-normalizer-3.1.0 click-8.1.3 idna-3.4 imageio-ffmpeg-0.4.3 ninja-1.11.1 numpy-1.24.3 pyspng-0.1.1 requests-2.31.0 tqdm-4.65.0 urllib3-2.0.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu110\n",
            "  Downloading https://download.pytorch.org/whl/cu110/torch-1.7.1%2Bcu110-cp38-cp38-linux_x86_64.whl (1156.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1156.8 MB 13 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu110\n",
            "  Downloading https://download.pytorch.org/whl/cu110/torchvision-0.8.2%2Bcu110-cp38-cp38-linux_x86_64.whl (12.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.9 MB 59.1 MB/s \n",
            "\u001b[?25hCollecting torchaudio==0.7.2\n",
            "  Downloading torchaudio-0.7.2-cp38-cp38-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 11.6 MB/s \n",
            "\u001b[?25hCollecting typing-extensions\n",
            "  Using cached typing_extensions-4.6.2-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch==1.7.1+cu110) (1.24.3)\n",
            "Collecting pillow>=4.1.1\n",
            "  Downloading Pillow-9.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 56.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: typing-extensions, torch, pillow, torchvision, torchaudio\n",
            "Successfully installed pillow-9.5.0 torch-1.7.1+cu110 torchaudio-0.7.2 torchvision-0.8.2+cu110 typing-extensions-4.6.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting psutil\n",
            "  Using cached psutil-5.9.5-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
            "Installing collected packages: psutil\n",
            "Successfully installed psutil-5.9.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 34.5 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.8/dist-packages (from scipy) (1.24.3)\n",
            "Installing collected packages: scipy\n",
            "Successfully installed scipy-1.10.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3-dev is already the newest version (3.8.2-0ubuntu2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Used GPU Info\n",
        "NVIDIA System Management Interface (nvidia-smi) is a command line utility based on top of the NVIDIA Management Library (NVML) intended to assist in the management and monitoring of NVIDIA GPU devices. ***This utility allows administrators to query GPU device state and, with appropriate privileges, administrators to change GPU device state. ***\n",
        "\n",
        "> The **`-L flag`** is used to list all available NVIDIA GPUs in the system."
      ],
      "metadata": {
        "id": "MIUwsz69Ifaa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYl-dJycHd9i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e34a1d3-5ca0-4feb-b9f9-64d6deeb5f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-b432c36f-4c3e-be53-d146-d68e9c23ebeb)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connecting to Google Drive Account\n",
        "---\n",
        "\n",
        "\n",
        "Training models and progress images like a possible disconnection\n",
        "If you don't lose your models by saving them to your Google Drive account to eliminate problems. In this way, you can continue training from where you left off in your model."
      ],
      "metadata": {
        "id": "vazvjHhfdtS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wKVC5Gb0iWHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9d31bbb-87c0-480a-bf3a-f6ef96228277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CHQ0n2z0WxI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "if os.path.isdir(\"/content/drive/My Drive/colab-sg2-ada\"):\n",
        "    %cd \"/content/drive/My Drive/colab-sg2-ada/stylegan2-ada\"\n",
        "else:\n",
        "    #install script\n",
        "    %cd \"/content/drive/My Drive/\"\n",
        "    !mkdir colab-sg2-ada\n",
        "    %cd colab-sg2-ada\n",
        "    !git clone https://github.com/NVlabs/stylegan2-ada-pytorch\n",
        "    %cd stylegan2-ada\n",
        "    !mkdir datasets\n",
        "    !mkdir results\n",
        "    !mkdir output"
      ],
      "metadata": {
        "id": "RLnb65ZitkQf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cf64407-1bed-4a85-8c3a-a4e4f332a4b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[Errno 2] No such file or directory: '/content/drive/My Drive/colab-sg2-ada/stylegan2-ada'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decompressing Pictures in Drive\n",
        "---\n",
        "It provides access to your images by decompressing the previously uploaded images to Google Drive."
      ],
      "metadata": {
        "id": "vYoZLzqR4K4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = \"/content/drive/MyDrive/Handbags-Preprocess-Data-png.zip\"\n",
        "!unzip {zip_path} -d /content/drive/MyDrive/colab-sg2-ada/datasets"
      ],
      "metadata": {
        "id": "APbeWDo_4C7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Parameters of the Model\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wubj7wuSiflI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### General settings\n",
        "---\n",
        "\n",
        ">**--outdir DIR:** Directory where training outputs will be saved. Training results, model weights, sampling images, TensorBoard logs, etc. is saved in this directory. ***(Is necessary!)***\n",
        "\n",
        ">**--gpus INT:** Number of GPUs to be used in training. You can set this parameter to use multiple GPUs. (default = 1gpu)\n",
        "\n",
        ">**--snap INT:** Autosave interval (in seconds). Model weights are automatically saved at specified time intervals. (default = 50 ticks)\n",
        "\n",
        ">**--seed INT:** Specifies the seed value used for random number generation. This value, when using the same seed value, the training process will generate the same random numbers each time. This ensures that the training results are reproducible.\n",
        "\n",
        ">**--metrics LIST:** List of additional metrics to be calculated during training. It can be separated by commas to specify multiple metrics. (default = fid50k_full)\n",
        "\n",
        ">* **Frechet Inception Distance (FID):**\n",
        ">* **Kernel Inception Distance (KID):**\n",
        ">* **Precision Recall:**\n",
        ">* **Perceptual Path Length (PPL):**\n",
        ">* **Inception Score (IS):**\n",
        "\n",
        ">**--metricdata PATH:** Source directory of additional metric data. This specifies the source index of the dataset from which the metrics are calculated.\n",
        "\n",
        "**--n:** Specifying this option does not perform the training process, only configuration and data validation.\n",
        "This option can be used to detect configuration errors and speed up the training process."
      ],
      "metadata": {
        "id": "WjGj7S-k-sEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Set Related Parameters\n",
        "---\n",
        ">**--data PATH:** The source directory of the training dataset. The image data to be used in the training should be in this directory.***(Required!)***\n",
        "\n",
        ">**--res INT:** Specifies the image resolution to be used during training. *(default: highest value available)*\n",
        "\n",
        ">>* Training at a higher resolution can give you more detailed and sharper results, but can increase training time and resource requirements. Training at a lower resolution may result in a faster training time and lower resource requirements, but you may get less detailed results.\n",
        "\n",
        ">**--mirror BOOL:** Mirroring image data on the horizontal axis (mirror display). This can be used as a data augmentation method. (defult: False)\n",
        "\n",
        ">**--mirrory BOOL:** Mirroring the image data on the vertical axis. This is also a data increment method. (default: False)\n",
        "\n",
        ">**--use-raw BOOL:** Determines whether to use raw images of the data to be used during training. *(default: False)*\n",
        "\n",
        ">>* StyleGAN2-ADA usually uses RGB (red, green, blue) channels of data for training. However, in some cases it may be necessary to use different channels of data (for example, thermal or depth data). The --use-raw parameter allows you to control this state.\n",
        "\n",
        ">>* If BOOL is set to True (--use-raw True), you can use raw images of data in training. This means that not only the RGB channels of the data will be used, but other channels as well.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tlhfvspuiCYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Configurations\n",
        "---\n",
        "\n",
        ">**--cfg {auto,11gb-gpu,11gb-gpu-complex,24gb-gpu,24gb-gpu-complex,48gb-gpu,stylegan2,paper256,paper512,paper1024,cifar,cifarbaseline}:** What to use Specifies the configuration (config) file. This file contains important configuration information such as architecture, hyperparameters, and other settings of the model. *(default = auto)*\n",
        "\n",
        ">**--gamma FLOAT:** The gamma value controls the rate at which the style weight is updated. The style weight represents the style properties of the images and transfers the style of the real images to the synthesized images during training.\n",
        "\n",
        "\n",
        ">>* A higher --gamma value causes the discriminator to become more dominant, encouraging the generator to produce more realistic images. However, a --gamma value that is too high can affect the model's learning process and result in over-responsive or low-diversity results.\n",
        "\n",
        ">>* A lower --gamma value allows the generator to have more control and allows it to produce more diverse images. However, a very low --gamma value should also be chosen with care, as this may reduce the effect of the discriminator and reduce the realism of the images produced.\n",
        "\n",
        ">**--kimg INT:** A hyperparameter that determines the training time. It is used to specify how many thousand images will be used in the tutorial.\n",
        "\n",
        ">>* Training of StyleGAN2-ADA model is carried out through a series of training steps (iteration). Each training step works with a mini-batch and updates the weights of the model. The training time is determined by the number of training steps.\n",
        "\n",
        ">**--batch:** Specifies the size of the data stacks (batches) used during training.\n",
        "\n",
        ">>* During training, because the dataset is often very large, it may not be possible to load and process all the data into memory at the same time. Therefore, the dataset is divided into small batches (batches) and each batch is processed separately.\n",
        "\n",
        ">>* Factors to consider when determining batch size include available memory and computational resources, size of your dataset, generalization ability, and training time.\n",
        "\n",
        ">>>> **1. Memory usage:** The larger the batch size, the more memory is used. Memory limitations should be considered, especially when working with a large dataset.\n",
        "\n",
        ">>>> **2. Calculation speed:** Batch size can affect the speed of training operations. Larger batches generally provide faster computations, but take more time per training step.\n",
        "\n",
        ">>>> **3. Generalization and diversity:*** Larger batches can make the model have better generalization ability. However, smaller batches can provide greater variety and encourage better model learning on different samples."
      ],
      "metadata": {
        "id": "sbmZkkgF_knE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation Parameters of Discriminator\n",
        "\n",
        ">**--aug {noaug,ada,fixed,adarv}:** Used to specify the data augmentation method. Data augmentation is a technique used to increase diversity in the training dataset.\n",
        "*(default = 'island')*\n",
        "\n",
        ">>* **noaug:** This value indicates that data augmentation will not be used. No changes are made to the data set.\n",
        "\n",
        ">>* **ada:** It uses Adaptive Data Augmentation (ADA) method. ADA uses an optimization algorithm to improve training by automatically applying data augmentation operations on the dataset. This method can provide a better generalization by highlighting different features of the dataset.\n",
        "\n",
        ">>* **fixed:** This value indicates that a fixed data augmentation strategy will be used. Data augmentation operations defined in a particular set are applied throughout the entire training process. For example, operations such as random rotation, truncation, or mirroring can be applied in a fixed manner.\n",
        "\n",
        ">>>* **--p FLOAT:** Specifies the possibility of data augmentation to be applied in 'fixed' mode.\n",
        "\n",
        ">>* **adarv:** Uses Adaptive Augmentations with Random Values (ADARV). This method automatically applies data augmentation based on the properties of the dataset. It can achieve greater diversity by applying different data augmentation strategies to different data points.\n",
        "\n",
        ">**--target FLOAT:** Dynamically adjusts the data boost rate by measuring the similarities between the real and produced images based on the data boost mode. *(default = depends on aug)*\n",
        "\n",
        ">>* When determining the parameter, it is important to consider the balance of diversity and similarity of the images produced by the model to real images.\n",
        ">>> 1. When a high target value (eg 0.7) is used, ADA can do more data increments. This means that the model will tend to produce images that are more similar to the actual images.\n",
        ">>> 2. Using a lower \"target\" value (eg 0.3) may result in less data augmentation from ADA. This means that the model will tend to produce more diverse and varied images.\n",
        "\n",
        ">**--augpipe {'blit', 'geom', 'color', 'filter', 'noise', 'cutout', 'bg', 'bgc', ..., 'bgcfnc'}:** Creates an array that specifies the data increment method. *(default = 'bgc')*\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yy0Zw2VwAIgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transfer Learning Parameters\n",
        "\n",
        "---\n",
        "\n",
        "> **--resume RESUME:** This parameter specifies the path of a saved model to resume training from where you stopped it. If you want to interrupt the training and continue later, you can load your saved model using this parameter.\n",
        "\n",
        "> **--freezed INT:** This parameter specifies the number of frozen layers in the model. Layers lower than the specified number are not updated during training and do not contribute to the learning process. This parameter can be used in transfer learning or fine-tuning scenarios.\n"
      ],
      "metadata": {
        "id": "-kmH52_jZ1hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/train.py --help"
      ],
      "metadata": {
        "id": "jgvPes7ugApB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b87a0db8-51a2-4149-cac4-0b9758f466a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: train.py [OPTIONS]\n",
            "\n",
            "  Train a GAN using the techniques described in\n",
            "  the paper \"Training Generative Adversarial\n",
            "  Networks with Limited Data\".\n",
            "\n",
            "  Examples:\n",
            "\n",
            "  # Train with custom dataset using 1 GPU.\n",
            "  python train.py --outdir=~/training-runs --data=~/mydataset.zip --gpus=1\n",
            "\n",
            "  # Train class-conditional CIFAR-10 using 2 GPUs.\n",
            "  python train.py --outdir=~/training-runs --data=~/datasets/cifar10.zip \\\n",
            "      --gpus=2 --cfg=cifar --cond=1\n",
            "\n",
            "  # Transfer learn MetFaces from FFHQ using 4 GPUs.\n",
            "  python train.py --outdir=~/training-runs --data=~/datasets/metfaces.zip \\\n",
            "      --gpus=4 --cfg=paper1024 --mirror=1 --resume=ffhq1024 --snap=10\n",
            "\n",
            "  # Reproduce original StyleGAN2 config F.\n",
            "  python train.py --outdir=~/training-runs --data=~/datasets/ffhq.zip \\\n",
            "      --gpus=8 --cfg=stylegan2 --mirror=1 --aug=noaug\n",
            "\n",
            "  Base configs (--cfg):\n",
            "    auto       Automatically select reasonable defaults based on resolution\n",
            "               and GPU count. Good starting point for new datasets.\n",
            "    stylegan2  Reproduce results for StyleGAN2 config F at 1024x1024.\n",
            "    paper256   Reproduce results for FFHQ and LSUN Cat at 256x256.\n",
            "    paper512   Reproduce results for BreCaHAD and AFHQ at 512x512.\n",
            "    paper1024  Reproduce results for MetFaces at 1024x1024.\n",
            "    cifar      Reproduce results for CIFAR-10 at 32x32.\n",
            "\n",
            "  Transfer learning source networks (--resume):\n",
            "    ffhq256        FFHQ trained at 256x256 resolution.\n",
            "    ffhq512        FFHQ trained at 512x512 resolution.\n",
            "    ffhq1024       FFHQ trained at 1024x1024 resolution.\n",
            "    celebahq256    CelebA-HQ trained at 256x256 resolution.\n",
            "    lsundog256     LSUN Dog trained at 256x256 resolution.\n",
            "    <PATH or URL>  Custom network pickle.\n",
            "\n",
            "Options:\n",
            "  --outdir DIR                    Where to save\n",
            "                                  the results\n",
            "                                  [required]\n",
            "  --gpus INT                      Number of GPUs\n",
            "                                  to use [default:\n",
            "                                  1]\n",
            "  --snap INT                      Snapshot\n",
            "                                  interval\n",
            "                                  [default: 50\n",
            "                                  ticks]\n",
            "  --metrics LIST                  Comma-separated\n",
            "                                  list or \"none\"\n",
            "                                  [default:\n",
            "                                  fid50k_full]\n",
            "  --seed INT                      Random seed\n",
            "                                  [default: 0]\n",
            "  -n, --dry-run                   Print training\n",
            "                                  options and exit\n",
            "  --data PATH                     Training data\n",
            "                                  (directory or\n",
            "                                  zip)  [required]\n",
            "  --cond BOOL                     Train\n",
            "                                  conditional\n",
            "                                  model based on\n",
            "                                  dataset labels\n",
            "                                  [default: false]\n",
            "  --subset INT                    Train with only\n",
            "                                  N images\n",
            "                                  [default: all]\n",
            "  --mirror BOOL                   Enable dataset\n",
            "                                  x-flips\n",
            "                                  [default: false]\n",
            "  --cfg [auto|stylegan2|paper256|paper512|paper1024|cifar]\n",
            "                                  Base config\n",
            "                                  [default: auto]\n",
            "  --gamma FLOAT                   Override R1\n",
            "                                  gamma\n",
            "  --kimg INT                      Override\n",
            "                                  training\n",
            "                                  duration\n",
            "  --batch INT                     Override batch\n",
            "                                  size\n",
            "  --aug [noaug|ada|fixed]         Augmentation\n",
            "                                  mode [default:\n",
            "                                  ada]\n",
            "  --p FLOAT                       Augmentation\n",
            "                                  probability for\n",
            "                                  --aug=fixed\n",
            "  --target FLOAT                  ADA target value\n",
            "                                  for --aug=ada\n",
            "  --augpipe [blit|geom|color|filter|noise|cutout|bg|bgc|bgcf|bgcfn|bgcfnc]\n",
            "                                  Augmentation\n",
            "                                  pipeline\n",
            "                                  [default: bgc]\n",
            "  --resume PKL                    Resume training\n",
            "                                  [default:\n",
            "                                  noresume]\n",
            "  --freezed INT                   Freeze-D\n",
            "                                  [default: 0\n",
            "                                  layers]\n",
            "  --fp32 BOOL                     Disable mixed-\n",
            "                                  precision\n",
            "                                  training\n",
            "  --nhwc BOOL                     Use NHWC memory\n",
            "                                  format with FP16\n",
            "  --nobench BOOL                  Disable cuDNN\n",
            "                                  benchmarking\n",
            "  --allow-tf32 BOOL               Allow PyTorch to\n",
            "                                  use TF32\n",
            "                                  internally\n",
            "  --workers INT                   Override number\n",
            "                                  of DataLoader\n",
            "                                  workers\n",
            "  --help                          Show this\n",
            "                                  message and\n",
            "                                  exit.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/train.py --outdir='/content/drive/MyDrive/colab-sg2-ada/results' --data='/content/drive/MyDrive/colab-sg2-ada/datasets/Handbags-Preprocess-Data-png' --gpus=1 --metrics='kid50k_full' --resume='/content/drive/MyDrive/colab-sg2-ada/results/00006-Handbags-Preprocess-Data-png-auto1-gamma30-kimg5000-batch16-ada-bg-resumecustom/network-snapshot-000960.pkl' --batch=16 --snap=10 --kimg=5000  --resume='' --aug='ada' --augpipe='bg' --gamma=30 --dry-run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vyP8XvVkNou",
        "outputId": "386007ed-c6f9-49e2-8428-66263e5d7b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training options:\n",
            "{\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 10,\n",
            "  \"network_snapshot_ticks\": 10,\n",
            "  \"metrics\": [\n",
            "    \"kid50k_full\"\n",
            "  ],\n",
            "  \"random_seed\": 0,\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"/content/drive/MyDrive/colab-sg2-ada/datasets/Handbags-Preprocess-Data-png\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 6498,\n",
            "    \"xflip\": false,\n",
            "    \"resolution\": 256\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"num_workers\": 3,\n",
            "    \"prefetch_factor\": 2\n",
            "  },\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 2\n",
            "    },\n",
            "    \"synthesis_kwargs\": {\n",
            "      \"channel_base\": 16384,\n",
            "      \"channel_max\": 512,\n",
            "      \"num_fp16_res\": 4,\n",
            "      \"conv_clamp\": 256\n",
            "    }\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Discriminator\",\n",
            "    \"block_kwargs\": {},\n",
            "    \"mapping_kwargs\": {},\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 4\n",
            "    },\n",
            "    \"channel_base\": 16384,\n",
            "    \"channel_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 30.0\n",
            "  },\n",
            "  \"total_kimg\": 5000,\n",
            "  \"batch_size\": 16,\n",
            "  \"batch_gpu\": 16,\n",
            "  \"ema_kimg\": 5.0,\n",
            "  \"ema_rampup\": null,\n",
            "  \"ada_target\": 0.6,\n",
            "  \"augment_kwargs\": {\n",
            "    \"class_name\": \"training.augment.AugmentPipe\",\n",
            "    \"xflip\": 1,\n",
            "    \"rotate90\": 1,\n",
            "    \"xint\": 1,\n",
            "    \"scale\": 1,\n",
            "    \"rotate\": 1,\n",
            "    \"aniso\": 1,\n",
            "    \"xfrac\": 1\n",
            "  },\n",
            "  \"resume_pkl\": \"\",\n",
            "  \"ada_kimg\": 100,\n",
            "  \"run_dir\": \"/content/drive/MyDrive/colab-sg2-ada/results/00010-Handbags-Preprocess-Data-png-auto1-gamma30-kimg5000-batch16-ada-bg-resumecustom\"\n",
            "}\n",
            "\n",
            "Output directory:   /content/drive/MyDrive/colab-sg2-ada/results/00010-Handbags-Preprocess-Data-png-auto1-gamma30-kimg5000-batch16-ada-bg-resumecustom\n",
            "Training data:      /content/drive/MyDrive/colab-sg2-ada/datasets/Handbags-Preprocess-Data-png\n",
            "Training duration:  5000 kimg\n",
            "Number of GPUs:     1\n",
            "Number of images:   6498\n",
            "Image resolution:   256\n",
            "Conditional model:  False\n",
            "Dataset x-flips:    False\n",
            "\n",
            "Dry run; exiting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/train.py --outdir='/content/drive/MyDrive/colab-sg2-ada/results' --data='/content/drive/MyDrive/colab-sg2-ada/datasets/Handbags-Preprocess-Data-png' --gpus=1 --metrics='kid50k_full' --batch=16 --snap=10 --kimg=5000  --resume='/content/drive/MyDrive/colab-sg2-ada/results/00006-Handbags-Preprocess-Data-png-auto1-gamma30-kimg5000-batch16-ada-bg-resumecustom/network-snapshot-000960.pkl' --aug='ada' --augpipe='bg' --gamma=30"
      ],
      "metadata": {
        "id": "CG_qPKar17oq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74929ebb-eaf8-48e3-f8bd-e77f28ccf84f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training options:\n",
            "{\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 10,\n",
            "  \"network_snapshot_ticks\": 10,\n",
            "  \"metrics\": [\n",
            "    \"kid50k_full\"\n",
            "  ],\n",
            "  \"random_seed\": 0,\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"/content/drive/MyDrive/colab-sg2-ada/datasets/Handbags-Preprocess-Data-png\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 6498,\n",
            "    \"xflip\": false,\n",
            "    \"resolution\": 256\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"num_workers\": 3,\n",
            "    \"prefetch_factor\": 2\n",
            "  },\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 2\n",
            "    },\n",
            "    \"synthesis_kwargs\": {\n",
            "      \"channel_base\": 16384,\n",
            "      \"channel_max\": 512,\n",
            "      \"num_fp16_res\": 4,\n",
            "      \"conv_clamp\": 256\n",
            "    }\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Discriminator\",\n",
            "    \"block_kwargs\": {},\n",
            "    \"mapping_kwargs\": {},\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 4\n",
            "    },\n",
            "    \"channel_base\": 16384,\n",
            "    \"channel_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 30.0\n",
            "  },\n",
            "  \"total_kimg\": 5000,\n",
            "  \"batch_size\": 16,\n",
            "  \"batch_gpu\": 16,\n",
            "  \"ema_kimg\": 5.0,\n",
            "  \"ema_rampup\": null,\n",
            "  \"ada_target\": 0.6,\n",
            "  \"augment_kwargs\": {\n",
            "    \"class_name\": \"training.augment.AugmentPipe\",\n",
            "    \"xflip\": 1,\n",
            "    \"rotate90\": 1,\n",
            "    \"xint\": 1,\n",
            "    \"scale\": 1,\n",
            "    \"rotate\": 1,\n",
            "    \"aniso\": 1,\n",
            "    \"xfrac\": 1\n",
            "  },\n",
            "  \"resume_pkl\": \"/content/drive/MyDrive/colab-sg2-ada/results/00006-Handbags-Preprocess-Data-png-auto1-gamma30-kimg5000-batch16-ada-bg-resumecustom/network-snapshot-000960.pkl\",\n",
            "  \"ada_kimg\": 100,\n",
            "  \"run_dir\": \"/content/drive/MyDrive/colab-sg2-ada/results/00011-Handbags-Preprocess-Data-png-auto1-gamma30-kimg5000-batch16-ada-bg-resumecustom\"\n",
            "}\n",
            "\n",
            "Output directory:   /content/drive/MyDrive/colab-sg2-ada/results/00011-Handbags-Preprocess-Data-png-auto1-gamma30-kimg5000-batch16-ada-bg-resumecustom\n",
            "Training data:      /content/drive/MyDrive/colab-sg2-ada/datasets/Handbags-Preprocess-Data-png\n",
            "Training duration:  5000 kimg\n",
            "Number of GPUs:     1\n",
            "Number of images:   6498\n",
            "Image resolution:   256\n",
            "Conditional model:  False\n",
            "Dataset x-flips:    False\n",
            "\n",
            "Creating output directory...\n",
            "Launching processes...\n",
            "Loading training set...\n",
            "\n",
            "Num images:  6498\n",
            "Image shape: [3, 256, 256]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Resuming from \"/content/drive/MyDrive/colab-sg2-ada/results/00006-Handbags-Preprocess-Data-png-auto1-gamma30-kimg5000-batch16-ada-bg-resumecustom/network-snapshot-000960.pkl\"\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "\n",
            "Generator             Parameters  Buffers  Output shape         Datatype\n",
            "---                   ---         ---      ---                  ---     \n",
            "mapping.fc0           262656      -        [16, 512]            float32 \n",
            "mapping.fc1           262656      -        [16, 512]            float32 \n",
            "mapping               -           512      [16, 14, 512]        float32 \n",
            "synthesis.b4.conv1    2622465     32       [16, 512, 4, 4]      float32 \n",
            "synthesis.b4.torgb    264195      -        [16, 3, 4, 4]        float32 \n",
            "synthesis.b4:0        8192        16       [16, 512, 4, 4]      float32 \n",
            "synthesis.b4:1        -           -        [16, 512, 4, 4]      float32 \n",
            "synthesis.b8.conv0    2622465     80       [16, 512, 8, 8]      float32 \n",
            "synthesis.b8.conv1    2622465     80       [16, 512, 8, 8]      float32 \n",
            "synthesis.b8.torgb    264195      -        [16, 3, 8, 8]        float32 \n",
            "synthesis.b8:0        -           16       [16, 512, 8, 8]      float32 \n",
            "synthesis.b8:1        -           -        [16, 512, 8, 8]      float32 \n",
            "synthesis.b16.conv0   2622465     272      [16, 512, 16, 16]    float32 \n",
            "synthesis.b16.conv1   2622465     272      [16, 512, 16, 16]    float32 \n",
            "synthesis.b16.torgb   264195      -        [16, 3, 16, 16]      float32 \n",
            "synthesis.b16:0       -           16       [16, 512, 16, 16]    float32 \n",
            "synthesis.b16:1       -           -        [16, 512, 16, 16]    float32 \n",
            "synthesis.b32.conv0   2622465     1040     [16, 512, 32, 32]    float16 \n",
            "synthesis.b32.conv1   2622465     1040     [16, 512, 32, 32]    float16 \n",
            "synthesis.b32.torgb   264195      -        [16, 3, 32, 32]      float16 \n",
            "synthesis.b32:0       -           16       [16, 512, 32, 32]    float16 \n",
            "synthesis.b32:1       -           -        [16, 512, 32, 32]    float32 \n",
            "synthesis.b64.conv0   1442561     4112     [16, 256, 64, 64]    float16 \n",
            "synthesis.b64.conv1   721409      4112     [16, 256, 64, 64]    float16 \n",
            "synthesis.b64.torgb   132099      -        [16, 3, 64, 64]      float16 \n",
            "synthesis.b64:0       -           16       [16, 256, 64, 64]    float16 \n",
            "synthesis.b64:1       -           -        [16, 256, 64, 64]    float32 \n",
            "synthesis.b128.conv0  426369      16400    [16, 128, 128, 128]  float16 \n",
            "synthesis.b128.conv1  213249      16400    [16, 128, 128, 128]  float16 \n",
            "synthesis.b128.torgb  66051       -        [16, 3, 128, 128]    float16 \n",
            "synthesis.b128:0      -           16       [16, 128, 128, 128]  float16 \n",
            "synthesis.b128:1      -           -        [16, 128, 128, 128]  float32 \n",
            "synthesis.b256.conv0  139457      65552    [16, 64, 256, 256]   float16 \n",
            "synthesis.b256.conv1  69761       65552    [16, 64, 256, 256]   float16 \n",
            "synthesis.b256.torgb  33027       -        [16, 3, 256, 256]    float16 \n",
            "synthesis.b256:0      -           16       [16, 64, 256, 256]   float16 \n",
            "synthesis.b256:1      -           -        [16, 64, 256, 256]   float32 \n",
            "---                   ---         ---      ---                  ---     \n",
            "Total                 23191522    175568   -                    -       \n",
            "\n",
            "\n",
            "Discriminator  Parameters  Buffers  Output shape         Datatype\n",
            "---            ---         ---      ---                  ---     \n",
            "b256.fromrgb   256         16       [16, 64, 256, 256]   float16 \n",
            "b256.skip      8192        16       [16, 128, 128, 128]  float16 \n",
            "b256.conv0     36928       16       [16, 64, 256, 256]   float16 \n",
            "b256.conv1     73856       16       [16, 128, 128, 128]  float16 \n",
            "b256           -           16       [16, 128, 128, 128]  float16 \n",
            "b128.skip      32768       16       [16, 256, 64, 64]    float16 \n",
            "b128.conv0     147584      16       [16, 128, 128, 128]  float16 \n",
            "b128.conv1     295168      16       [16, 256, 64, 64]    float16 \n",
            "b128           -           16       [16, 256, 64, 64]    float16 \n",
            "b64.skip       131072      16       [16, 512, 32, 32]    float16 \n",
            "b64.conv0      590080      16       [16, 256, 64, 64]    float16 \n",
            "b64.conv1      1180160     16       [16, 512, 32, 32]    float16 \n",
            "b64            -           16       [16, 512, 32, 32]    float16 \n",
            "b32.skip       262144      16       [16, 512, 16, 16]    float16 \n",
            "b32.conv0      2359808     16       [16, 512, 32, 32]    float16 \n",
            "b32.conv1      2359808     16       [16, 512, 16, 16]    float16 \n",
            "b32            -           16       [16, 512, 16, 16]    float16 \n",
            "b16.skip       262144      16       [16, 512, 8, 8]      float32 \n",
            "b16.conv0      2359808     16       [16, 512, 16, 16]    float32 \n",
            "b16.conv1      2359808     16       [16, 512, 8, 8]      float32 \n",
            "b16            -           16       [16, 512, 8, 8]      float32 \n",
            "b8.skip        262144      16       [16, 512, 4, 4]      float32 \n",
            "b8.conv0       2359808     16       [16, 512, 8, 8]      float32 \n",
            "b8.conv1       2359808     16       [16, 512, 4, 4]      float32 \n",
            "b8             -           16       [16, 512, 4, 4]      float32 \n",
            "b4.mbstd       -           -        [16, 513, 4, 4]      float32 \n",
            "b4.conv        2364416     16       [16, 512, 4, 4]      float32 \n",
            "b4.fc          4194816     -        [16, 512]            float32 \n",
            "b4.out         513         -        [16, 1]              float32 \n",
            "---            ---         ---      ---                  ---     \n",
            "Total          24001089    416      -                    -       \n",
            "\n",
            "Setting up augmentation...\n",
            "Distributing across 1 GPUs...\n",
            "Setting up training phases...\n",
            "Exporting sample images...\n",
            "Initializing logs...\n",
            "Skipping tfevents export: No module named 'tensorboard'\n",
            "Training for 5000 kimg...\n",
            "\n",
            "tick 0     kimg 0.0      time 1m 45s       sec/tick 4.7     sec/kimg 292.65  maintenance 100.6  cpumem 4.40   gpumem 11.25  augment 0.000\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"kid50k_full\": 0.002691549927427395}, \"metric\": \"kid50k_full\", \"total_time\": 402.50265431404114, \"total_time_str\": \"6m 43s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000000.pkl\", \"timestamp\": 1685333524.917549}\n",
            "tick 1     kimg 4.0      time 10m 14s      sec/tick 91.3    sec/kimg 22.83   maintenance 417.2  cpumem 4.76   gpumem 5.37   augment 0.022\n",
            "tick 2     kimg 8.0      time 11m 45s      sec/tick 91.5    sec/kimg 22.87   maintenance 0.0    cpumem 4.76   gpumem 4.91   augment 0.038\n",
            "tick 3     kimg 12.0     time 13m 17s      sec/tick 92.1    sec/kimg 23.04   maintenance 0.0    cpumem 4.76   gpumem 4.91   augment 0.054\n",
            "tick 4     kimg 16.0     time 14m 48s      sec/tick 90.8    sec/kimg 22.71   maintenance 0.0    cpumem 4.76   gpumem 4.91   augment 0.068\n",
            "tick 5     kimg 20.0     time 16m 20s      sec/tick 92.1    sec/kimg 23.02   maintenance 0.0    cpumem 4.76   gpumem 4.91   augment 0.080\n",
            "tick 6     kimg 24.0     time 17m 52s      sec/tick 91.1    sec/kimg 22.78   maintenance 0.0    cpumem 4.76   gpumem 4.91   augment 0.090\n",
            "tick 7     kimg 28.0     time 19m 24s      sec/tick 92.5    sec/kimg 23.12   maintenance 0.0    cpumem 4.76   gpumem 4.92   augment 0.103\n",
            "tick 8     kimg 32.0     time 20m 55s      sec/tick 91.1    sec/kimg 22.77   maintenance 0.0    cpumem 4.76   gpumem 4.91   augment 0.109\n",
            "tick 9     kimg 36.0     time 22m 27s      sec/tick 91.8    sec/kimg 22.95   maintenance 0.0    cpumem 4.76   gpumem 4.91   augment 0.116\n",
            "tick 10    kimg 40.0     time 23m 58s      sec/tick 91.0    sec/kimg 22.75   maintenance 0.0    cpumem 4.76   gpumem 4.91   augment 0.120\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"kid50k_full\": 0.00287169182182181}, \"metric\": \"kid50k_full\", \"total_time\": 351.8890357017517, \"total_time_str\": \"5m 52s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000040.pkl\", \"timestamp\": 1685334808.0037894}\n",
            "tick 11    kimg 44.0     time 31m 37s      sec/tick 91.2    sec/kimg 22.80   maintenance 367.5  cpumem 4.76   gpumem 4.91   augment 0.124\n",
            "tick 12    kimg 48.0     time 33m 08s      sec/tick 91.5    sec/kimg 22.87   maintenance 0.0    cpumem 4.76   gpumem 4.91   augment 0.120\n",
            "tick 13    kimg 52.0     time 34m 40s      sec/tick 91.9    sec/kimg 22.97   maintenance 0.0    cpumem 4.76   gpumem 4.91   augment 0.120\n",
            "tick 14    kimg 56.0     time 36m 12s      sec/tick 91.6    sec/kimg 22.90   maintenance 0.0    cpumem 4.76   gpumem 4.91   augment 0.130\n",
            "tick 15    kimg 60.0     time 37m 44s      sec/tick 92.1    sec/kimg 23.04   maintenance 0.0    cpumem 4.76   gpumem 4.91   augment 0.131\n",
            "tick 16    kimg 64.0     time 39m 16s      sec/tick 92.0    sec/kimg 23.01   maintenance 0.0    cpumem 4.76   gpumem 4.94   augment 0.131\n",
            "tick 17    kimg 68.0     time 40m 48s      sec/tick 92.2    sec/kimg 23.04   maintenance 0.0    cpumem 4.76   gpumem 4.91   augment 0.135\n",
            "tick 18    kimg 72.0     time 42m 21s      sec/tick 92.4    sec/kimg 23.11   maintenance 0.0    cpumem 4.76   gpumem 4.91   augment 0.141\n",
            "tick 19    kimg 76.0     time 43m 52s      sec/tick 91.9    sec/kimg 22.97   maintenance 0.0    cpumem 4.76   gpumem 4.91   augment 0.141\n",
            "tick 20    kimg 80.0     time 45m 25s      sec/tick 92.8    sec/kimg 23.19   maintenance 0.0    cpumem 4.76   gpumem 4.91   augment 0.143\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"kid50k_full\": 0.0026946235460460367}, \"metric\": \"kid50k_full\", \"total_time\": 359.3728361129761, \"total_time_str\": \"5m 59s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000080.pkl\", \"timestamp\": 1685336101.638571}\n",
            "\n",
            "Aborted!\n"
          ]
        }
      ]
    }
  ]
}